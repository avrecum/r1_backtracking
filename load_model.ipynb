{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer in a memory-efficient way\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model with memory efficiency options for inference only\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 precision\n",
    "    low_cpu_mem_usage=True,  # Optimize CPU memory usage\n",
    ").to(device)  # Explicitly move to CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Get a random sample from the dataset\n",
    "sample_idx = random.randint(0, len(dataset[\"train\"]) - 1)\n",
    "sample = dataset[\"train\"][sample_idx]\n",
    "\n",
    "print(\"Sample problem:\")\n",
    "print(sample[\"question\"])\n",
    "print(\"\\nReference answer:\")\n",
    "print(sample[\"reference_answer\"])\n",
    "\n",
    "# Prepare the prompt for the model\n",
    "prompt = f\"Question: {sample['question']}\\n\\nSolve this step-by-step:\"\n",
    "\n",
    "# Tokenize the input and move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate the solution\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=2048,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the model's solution\n",
    "print(\"\\nModel's solution:\")\n",
    "print(generated_text[len(prompt):])  # Remove the prompt from the output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
