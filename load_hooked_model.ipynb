{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the hooked transformer model. We are using the Llama 3 8B architecture, and passing the distilled reasoning model as the HF model because HookedTransformer doesn't support the actual Distill-Llama model yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading DeepSeek-R1-Distill-Llama-8B with HookedTransformer...\n",
      "Loading model with HuggingFace first...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4306472dc0148b484ecf337ea80d44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B into HookedTransformer\n",
      "Model loaded successfully: deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
      "Model has 32 layers and 32 attention heads\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the model using HookedTransformer\n",
    "print(\"Loading DeepSeek-R1-Distill-Llama-8B with HookedTransformer...\")\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# First load the model with HuggingFace to pass to HookedTransformer\n",
    "print(\"Loading model with HuggingFace first...\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 precision for memory efficiency\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Now load with HookedTransformer using the HuggingFace model\n",
    "model = HookedTransformer.from_pretrained_no_processing(\n",
    "    \"meta-llama/Meta-Llama-3-8B\",  # Use Llama-3-8B architecture as base\n",
    "    hf_model=hf_model,             # Pass the actual model weights\n",
    "    device=device,\n",
    "    dtype=torch.bfloat16,\n",
    "    tokenizer=tokenizer,\n",
    "    # center_writing_weights=False,  # Don't try to center weights\n",
    "    # center_unembed=False,          # Don't try to center unembed\n",
    "    # fold_ln=False,                 # Use LayerNorm\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully: {model_name}\")\n",
    "print(f\"Model has {model.cfg.n_layers} layers and {model.cfg.n_heads} attention heads\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model with cache for prompt 1/5:\n",
      "'What is the solution of (x-3)(x+4)(x^3+1)?\n",
      "\n",
      "So the...'\n",
      "Tokenized input shape: torch.Size([1, 89])\n",
      "Completed processing prompt 1\n",
      "\n",
      "Running model with cache for prompt 2/5:\n",
      "'Compute the derivative of f(x) = x^3 + 2x^2 - 5x +...'\n",
      "Tokenized input shape: torch.Size([1, 124])\n",
      "Completed processing prompt 2\n",
      "\n",
      "Running model with cache for prompt 3/5:\n",
      "'Compute the derivative of f(x) = x^3 + 2x^2 - 5x +...'\n",
      "Tokenized input shape: torch.Size([1, 124])\n",
      "Completed processing prompt 3\n",
      "\n",
      "Running model with cache for prompt 4/5:\n",
      "'Compute the derivative of f(x) = 10^6 x^5 - 672,58...'\n",
      "Tokenized input shape: torch.Size([1, 251])\n",
      "Completed processing prompt 4\n",
      "\n",
      "Running model with cache for prompt 5/5:\n",
      "'Compute the derivative of f(x) = 10^6 x^5 - 672,58...'\n",
      "Tokenized input shape: torch.Size([1, 251])\n",
      "Completed processing prompt 5\n",
      "\n",
      "Cache contains the following activation types (from first prompt):\n",
      "- hook_embed\n",
      "- blocks.0.hook_resid_pre\n",
      "- blocks.0.ln1.hook_scale\n",
      "- blocks.0.ln1.hook_normalized\n",
      "- blocks.0.attn.hook_q\n",
      "- blocks.0.attn.hook_k\n",
      "- blocks.0.attn.hook_v\n",
      "- blocks.0.attn.hook_rot_q\n",
      "- blocks.0.attn.hook_rot_k\n",
      "- blocks.0.attn.hook_attn_scores\n",
      "- blocks.0.attn.hook_pattern\n",
      "- blocks.0.attn.hook_z\n",
      "- blocks.0.hook_attn_out\n",
      "- blocks.0.hook_resid_mid\n",
      "- blocks.0.ln2.hook_scale\n",
      "- blocks.0.ln2.hook_normalized\n",
      "- blocks.0.mlp.hook_pre\n",
      "- blocks.0.mlp.hook_pre_linear\n",
      "- blocks.0.mlp.hook_post\n",
      "- blocks.0.hook_mlp_out\n",
      "- blocks.0.hook_resid_post\n",
      "- blocks.1.hook_resid_pre\n",
      "- blocks.1.ln1.hook_scale\n",
      "- blocks.1.ln1.hook_normalized\n",
      "- blocks.1.attn.hook_q\n",
      "- blocks.1.attn.hook_k\n",
      "- blocks.1.attn.hook_v\n",
      "- blocks.1.attn.hook_rot_q\n",
      "- blocks.1.attn.hook_rot_k\n",
      "- blocks.1.attn.hook_attn_scores\n",
      "- blocks.1.attn.hook_pattern\n",
      "- blocks.1.attn.hook_z\n",
      "- blocks.1.hook_attn_out\n",
      "- blocks.1.hook_resid_mid\n",
      "- blocks.1.ln2.hook_scale\n",
      "- blocks.1.ln2.hook_normalized\n",
      "- blocks.1.mlp.hook_pre\n",
      "- blocks.1.mlp.hook_pre_linear\n",
      "- blocks.1.mlp.hook_post\n",
      "- blocks.1.hook_mlp_out\n",
      "- blocks.1.hook_resid_post\n",
      "- blocks.2.hook_resid_pre\n",
      "- blocks.2.ln1.hook_scale\n",
      "- blocks.2.ln1.hook_normalized\n",
      "- blocks.2.attn.hook_q\n",
      "- blocks.2.attn.hook_k\n",
      "- blocks.2.attn.hook_v\n",
      "- blocks.2.attn.hook_rot_q\n",
      "- blocks.2.attn.hook_rot_k\n",
      "- blocks.2.attn.hook_attn_scores\n",
      "- blocks.2.attn.hook_pattern\n",
      "- blocks.2.attn.hook_z\n",
      "- blocks.2.hook_attn_out\n",
      "- blocks.2.hook_resid_mid\n",
      "- blocks.2.ln2.hook_scale\n",
      "- blocks.2.ln2.hook_normalized\n",
      "- blocks.2.mlp.hook_pre\n",
      "- blocks.2.mlp.hook_pre_linear\n",
      "- blocks.2.mlp.hook_post\n",
      "- blocks.2.hook_mlp_out\n",
      "- blocks.2.hook_resid_post\n",
      "- blocks.3.hook_resid_pre\n",
      "- blocks.3.ln1.hook_scale\n",
      "- blocks.3.ln1.hook_normalized\n",
      "- blocks.3.attn.hook_q\n",
      "- blocks.3.attn.hook_k\n",
      "- blocks.3.attn.hook_v\n",
      "- blocks.3.attn.hook_rot_q\n",
      "- blocks.3.attn.hook_rot_k\n",
      "- blocks.3.attn.hook_attn_scores\n",
      "- blocks.3.attn.hook_pattern\n",
      "- blocks.3.attn.hook_z\n",
      "- blocks.3.hook_attn_out\n",
      "- blocks.3.hook_resid_mid\n",
      "- blocks.3.ln2.hook_scale\n",
      "- blocks.3.ln2.hook_normalized\n",
      "- blocks.3.mlp.hook_pre\n",
      "- blocks.3.mlp.hook_pre_linear\n",
      "- blocks.3.mlp.hook_post\n",
      "- blocks.3.hook_mlp_out\n",
      "- blocks.3.hook_resid_post\n",
      "- blocks.4.hook_resid_pre\n",
      "- blocks.4.ln1.hook_scale\n",
      "- blocks.4.ln1.hook_normalized\n",
      "- blocks.4.attn.hook_q\n",
      "- blocks.4.attn.hook_k\n",
      "- blocks.4.attn.hook_v\n",
      "- blocks.4.attn.hook_rot_q\n",
      "- blocks.4.attn.hook_rot_k\n",
      "- blocks.4.attn.hook_attn_scores\n",
      "- blocks.4.attn.hook_pattern\n",
      "- blocks.4.attn.hook_z\n",
      "- blocks.4.hook_attn_out\n",
      "- blocks.4.hook_resid_mid\n",
      "- blocks.4.ln2.hook_scale\n",
      "- blocks.4.ln2.hook_normalized\n",
      "- blocks.4.mlp.hook_pre\n",
      "- blocks.4.mlp.hook_pre_linear\n",
      "- blocks.4.mlp.hook_post\n",
      "- blocks.4.hook_mlp_out\n",
      "- blocks.4.hook_resid_post\n",
      "- blocks.5.hook_resid_pre\n",
      "- blocks.5.ln1.hook_scale\n",
      "- blocks.5.ln1.hook_normalized\n",
      "- blocks.5.attn.hook_q\n",
      "- blocks.5.attn.hook_k\n",
      "- blocks.5.attn.hook_v\n",
      "- blocks.5.attn.hook_rot_q\n",
      "- blocks.5.attn.hook_rot_k\n",
      "- blocks.5.attn.hook_attn_scores\n",
      "- blocks.5.attn.hook_pattern\n",
      "- blocks.5.attn.hook_z\n",
      "- blocks.5.hook_attn_out\n",
      "- blocks.5.hook_resid_mid\n",
      "- blocks.5.ln2.hook_scale\n",
      "- blocks.5.ln2.hook_normalized\n",
      "- blocks.5.mlp.hook_pre\n",
      "- blocks.5.mlp.hook_pre_linear\n",
      "- blocks.5.mlp.hook_post\n",
      "- blocks.5.hook_mlp_out\n",
      "- blocks.5.hook_resid_post\n",
      "- blocks.6.hook_resid_pre\n",
      "- blocks.6.ln1.hook_scale\n",
      "- blocks.6.ln1.hook_normalized\n",
      "- blocks.6.attn.hook_q\n",
      "- blocks.6.attn.hook_k\n",
      "- blocks.6.attn.hook_v\n",
      "- blocks.6.attn.hook_rot_q\n",
      "- blocks.6.attn.hook_rot_k\n",
      "- blocks.6.attn.hook_attn_scores\n",
      "- blocks.6.attn.hook_pattern\n",
      "- blocks.6.attn.hook_z\n",
      "- blocks.6.hook_attn_out\n",
      "- blocks.6.hook_resid_mid\n",
      "- blocks.6.ln2.hook_scale\n",
      "- blocks.6.ln2.hook_normalized\n",
      "- blocks.6.mlp.hook_pre\n",
      "- blocks.6.mlp.hook_pre_linear\n",
      "- blocks.6.mlp.hook_post\n",
      "- blocks.6.hook_mlp_out\n",
      "- blocks.6.hook_resid_post\n",
      "- blocks.7.hook_resid_pre\n",
      "- blocks.7.ln1.hook_scale\n",
      "- blocks.7.ln1.hook_normalized\n",
      "- blocks.7.attn.hook_q\n",
      "- blocks.7.attn.hook_k\n",
      "- blocks.7.attn.hook_v\n",
      "- blocks.7.attn.hook_rot_q\n",
      "- blocks.7.attn.hook_rot_k\n",
      "- blocks.7.attn.hook_attn_scores\n",
      "- blocks.7.attn.hook_pattern\n",
      "- blocks.7.attn.hook_z\n",
      "- blocks.7.hook_attn_out\n",
      "- blocks.7.hook_resid_mid\n",
      "- blocks.7.ln2.hook_scale\n",
      "- blocks.7.ln2.hook_normalized\n",
      "- blocks.7.mlp.hook_pre\n",
      "- blocks.7.mlp.hook_pre_linear\n",
      "- blocks.7.mlp.hook_post\n",
      "- blocks.7.hook_mlp_out\n",
      "- blocks.7.hook_resid_post\n",
      "- blocks.8.hook_resid_pre\n",
      "- blocks.8.ln1.hook_scale\n",
      "- blocks.8.ln1.hook_normalized\n",
      "- blocks.8.attn.hook_q\n",
      "- blocks.8.attn.hook_k\n",
      "- blocks.8.attn.hook_v\n",
      "- blocks.8.attn.hook_rot_q\n",
      "- blocks.8.attn.hook_rot_k\n",
      "- blocks.8.attn.hook_attn_scores\n",
      "- blocks.8.attn.hook_pattern\n",
      "- blocks.8.attn.hook_z\n",
      "- blocks.8.hook_attn_out\n",
      "- blocks.8.hook_resid_mid\n",
      "- blocks.8.ln2.hook_scale\n",
      "- blocks.8.ln2.hook_normalized\n",
      "- blocks.8.mlp.hook_pre\n",
      "- blocks.8.mlp.hook_pre_linear\n",
      "- blocks.8.mlp.hook_post\n",
      "- blocks.8.hook_mlp_out\n",
      "- blocks.8.hook_resid_post\n",
      "- blocks.9.hook_resid_pre\n",
      "- blocks.9.ln1.hook_scale\n",
      "- blocks.9.ln1.hook_normalized\n",
      "- blocks.9.attn.hook_q\n",
      "- blocks.9.attn.hook_k\n",
      "- blocks.9.attn.hook_v\n",
      "- blocks.9.attn.hook_rot_q\n",
      "- blocks.9.attn.hook_rot_k\n",
      "- blocks.9.attn.hook_attn_scores\n",
      "- blocks.9.attn.hook_pattern\n",
      "- blocks.9.attn.hook_z\n",
      "- blocks.9.hook_attn_out\n",
      "- blocks.9.hook_resid_mid\n",
      "- blocks.9.ln2.hook_scale\n",
      "- blocks.9.ln2.hook_normalized\n",
      "- blocks.9.mlp.hook_pre\n",
      "- blocks.9.mlp.hook_pre_linear\n",
      "- blocks.9.mlp.hook_post\n",
      "- blocks.9.hook_mlp_out\n",
      "- blocks.9.hook_resid_post\n",
      "- blocks.10.hook_resid_pre\n",
      "- blocks.10.ln1.hook_scale\n",
      "- blocks.10.ln1.hook_normalized\n",
      "- blocks.10.attn.hook_q\n",
      "- blocks.10.attn.hook_k\n",
      "- blocks.10.attn.hook_v\n",
      "- blocks.10.attn.hook_rot_q\n",
      "- blocks.10.attn.hook_rot_k\n",
      "- blocks.10.attn.hook_attn_scores\n",
      "- blocks.10.attn.hook_pattern\n",
      "- blocks.10.attn.hook_z\n",
      "- blocks.10.hook_attn_out\n",
      "- blocks.10.hook_resid_mid\n",
      "- blocks.10.ln2.hook_scale\n",
      "- blocks.10.ln2.hook_normalized\n",
      "- blocks.10.mlp.hook_pre\n",
      "- blocks.10.mlp.hook_pre_linear\n",
      "- blocks.10.mlp.hook_post\n",
      "- blocks.10.hook_mlp_out\n",
      "- blocks.10.hook_resid_post\n",
      "- blocks.11.hook_resid_pre\n",
      "- blocks.11.ln1.hook_scale\n",
      "- blocks.11.ln1.hook_normalized\n",
      "- blocks.11.attn.hook_q\n",
      "- blocks.11.attn.hook_k\n",
      "- blocks.11.attn.hook_v\n",
      "- blocks.11.attn.hook_rot_q\n",
      "- blocks.11.attn.hook_rot_k\n",
      "- blocks.11.attn.hook_attn_scores\n",
      "- blocks.11.attn.hook_pattern\n",
      "- blocks.11.attn.hook_z\n",
      "- blocks.11.hook_attn_out\n",
      "- blocks.11.hook_resid_mid\n",
      "- blocks.11.ln2.hook_scale\n",
      "- blocks.11.ln2.hook_normalized\n",
      "- blocks.11.mlp.hook_pre\n",
      "- blocks.11.mlp.hook_pre_linear\n",
      "- blocks.11.mlp.hook_post\n",
      "- blocks.11.hook_mlp_out\n",
      "- blocks.11.hook_resid_post\n",
      "- blocks.12.hook_resid_pre\n",
      "- blocks.12.ln1.hook_scale\n",
      "- blocks.12.ln1.hook_normalized\n",
      "- blocks.12.attn.hook_q\n",
      "- blocks.12.attn.hook_k\n",
      "- blocks.12.attn.hook_v\n",
      "- blocks.12.attn.hook_rot_q\n",
      "- blocks.12.attn.hook_rot_k\n",
      "- blocks.12.attn.hook_attn_scores\n",
      "- blocks.12.attn.hook_pattern\n",
      "- blocks.12.attn.hook_z\n",
      "- blocks.12.hook_attn_out\n",
      "- blocks.12.hook_resid_mid\n",
      "- blocks.12.ln2.hook_scale\n",
      "- blocks.12.ln2.hook_normalized\n",
      "- blocks.12.mlp.hook_pre\n",
      "- blocks.12.mlp.hook_pre_linear\n",
      "- blocks.12.mlp.hook_post\n",
      "- blocks.12.hook_mlp_out\n",
      "- blocks.12.hook_resid_post\n",
      "- blocks.13.hook_resid_pre\n",
      "- blocks.13.ln1.hook_scale\n",
      "- blocks.13.ln1.hook_normalized\n",
      "- blocks.13.attn.hook_q\n",
      "- blocks.13.attn.hook_k\n",
      "- blocks.13.attn.hook_v\n",
      "- blocks.13.attn.hook_rot_q\n",
      "- blocks.13.attn.hook_rot_k\n",
      "- blocks.13.attn.hook_attn_scores\n",
      "- blocks.13.attn.hook_pattern\n",
      "- blocks.13.attn.hook_z\n",
      "- blocks.13.hook_attn_out\n",
      "- blocks.13.hook_resid_mid\n",
      "- blocks.13.ln2.hook_scale\n",
      "- blocks.13.ln2.hook_normalized\n",
      "- blocks.13.mlp.hook_pre\n",
      "- blocks.13.mlp.hook_pre_linear\n",
      "- blocks.13.mlp.hook_post\n",
      "- blocks.13.hook_mlp_out\n",
      "- blocks.13.hook_resid_post\n",
      "- blocks.14.hook_resid_pre\n",
      "- blocks.14.ln1.hook_scale\n",
      "- blocks.14.ln1.hook_normalized\n",
      "- blocks.14.attn.hook_q\n",
      "- blocks.14.attn.hook_k\n",
      "- blocks.14.attn.hook_v\n",
      "- blocks.14.attn.hook_rot_q\n",
      "- blocks.14.attn.hook_rot_k\n",
      "- blocks.14.attn.hook_attn_scores\n",
      "- blocks.14.attn.hook_pattern\n",
      "- blocks.14.attn.hook_z\n",
      "- blocks.14.hook_attn_out\n",
      "- blocks.14.hook_resid_mid\n",
      "- blocks.14.ln2.hook_scale\n",
      "- blocks.14.ln2.hook_normalized\n",
      "- blocks.14.mlp.hook_pre\n",
      "- blocks.14.mlp.hook_pre_linear\n",
      "- blocks.14.mlp.hook_post\n",
      "- blocks.14.hook_mlp_out\n",
      "- blocks.14.hook_resid_post\n",
      "- blocks.15.hook_resid_pre\n",
      "- blocks.15.ln1.hook_scale\n",
      "- blocks.15.ln1.hook_normalized\n",
      "- blocks.15.attn.hook_q\n",
      "- blocks.15.attn.hook_k\n",
      "- blocks.15.attn.hook_v\n",
      "- blocks.15.attn.hook_rot_q\n",
      "- blocks.15.attn.hook_rot_k\n",
      "- blocks.15.attn.hook_attn_scores\n",
      "- blocks.15.attn.hook_pattern\n",
      "- blocks.15.attn.hook_z\n",
      "- blocks.15.hook_attn_out\n",
      "- blocks.15.hook_resid_mid\n",
      "- blocks.15.ln2.hook_scale\n",
      "- blocks.15.ln2.hook_normalized\n",
      "- blocks.15.mlp.hook_pre\n",
      "- blocks.15.mlp.hook_pre_linear\n",
      "- blocks.15.mlp.hook_post\n",
      "- blocks.15.hook_mlp_out\n",
      "- blocks.15.hook_resid_post\n",
      "- blocks.16.hook_resid_pre\n",
      "- blocks.16.ln1.hook_scale\n",
      "- blocks.16.ln1.hook_normalized\n",
      "- blocks.16.attn.hook_q\n",
      "- blocks.16.attn.hook_k\n",
      "- blocks.16.attn.hook_v\n",
      "- blocks.16.attn.hook_rot_q\n",
      "- blocks.16.attn.hook_rot_k\n",
      "- blocks.16.attn.hook_attn_scores\n",
      "- blocks.16.attn.hook_pattern\n",
      "- blocks.16.attn.hook_z\n",
      "- blocks.16.hook_attn_out\n",
      "- blocks.16.hook_resid_mid\n",
      "- blocks.16.ln2.hook_scale\n",
      "- blocks.16.ln2.hook_normalized\n",
      "- blocks.16.mlp.hook_pre\n",
      "- blocks.16.mlp.hook_pre_linear\n",
      "- blocks.16.mlp.hook_post\n",
      "- blocks.16.hook_mlp_out\n",
      "- blocks.16.hook_resid_post\n",
      "- blocks.17.hook_resid_pre\n",
      "- blocks.17.ln1.hook_scale\n",
      "- blocks.17.ln1.hook_normalized\n",
      "- blocks.17.attn.hook_q\n",
      "- blocks.17.attn.hook_k\n",
      "- blocks.17.attn.hook_v\n",
      "- blocks.17.attn.hook_rot_q\n",
      "- blocks.17.attn.hook_rot_k\n",
      "- blocks.17.attn.hook_attn_scores\n",
      "- blocks.17.attn.hook_pattern\n",
      "- blocks.17.attn.hook_z\n",
      "- blocks.17.hook_attn_out\n",
      "- blocks.17.hook_resid_mid\n",
      "- blocks.17.ln2.hook_scale\n",
      "- blocks.17.ln2.hook_normalized\n",
      "- blocks.17.mlp.hook_pre\n",
      "- blocks.17.mlp.hook_pre_linear\n",
      "- blocks.17.mlp.hook_post\n",
      "- blocks.17.hook_mlp_out\n",
      "- blocks.17.hook_resid_post\n",
      "- blocks.18.hook_resid_pre\n",
      "- blocks.18.ln1.hook_scale\n",
      "- blocks.18.ln1.hook_normalized\n",
      "- blocks.18.attn.hook_q\n",
      "- blocks.18.attn.hook_k\n",
      "- blocks.18.attn.hook_v\n",
      "- blocks.18.attn.hook_rot_q\n",
      "- blocks.18.attn.hook_rot_k\n",
      "- blocks.18.attn.hook_attn_scores\n",
      "- blocks.18.attn.hook_pattern\n",
      "- blocks.18.attn.hook_z\n",
      "- blocks.18.hook_attn_out\n",
      "- blocks.18.hook_resid_mid\n",
      "- blocks.18.ln2.hook_scale\n",
      "- blocks.18.ln2.hook_normalized\n",
      "- blocks.18.mlp.hook_pre\n",
      "- blocks.18.mlp.hook_pre_linear\n",
      "- blocks.18.mlp.hook_post\n",
      "- blocks.18.hook_mlp_out\n",
      "- blocks.18.hook_resid_post\n",
      "- blocks.19.hook_resid_pre\n",
      "- blocks.19.ln1.hook_scale\n",
      "- blocks.19.ln1.hook_normalized\n",
      "- blocks.19.attn.hook_q\n",
      "- blocks.19.attn.hook_k\n",
      "- blocks.19.attn.hook_v\n",
      "- blocks.19.attn.hook_rot_q\n",
      "- blocks.19.attn.hook_rot_k\n",
      "- blocks.19.attn.hook_attn_scores\n",
      "- blocks.19.attn.hook_pattern\n",
      "- blocks.19.attn.hook_z\n",
      "- blocks.19.hook_attn_out\n",
      "- blocks.19.hook_resid_mid\n",
      "- blocks.19.ln2.hook_scale\n",
      "- blocks.19.ln2.hook_normalized\n",
      "- blocks.19.mlp.hook_pre\n",
      "- blocks.19.mlp.hook_pre_linear\n",
      "- blocks.19.mlp.hook_post\n",
      "- blocks.19.hook_mlp_out\n",
      "- blocks.19.hook_resid_post\n",
      "- blocks.20.hook_resid_pre\n",
      "- blocks.20.ln1.hook_scale\n",
      "- blocks.20.ln1.hook_normalized\n",
      "- blocks.20.attn.hook_q\n",
      "- blocks.20.attn.hook_k\n",
      "- blocks.20.attn.hook_v\n",
      "- blocks.20.attn.hook_rot_q\n",
      "- blocks.20.attn.hook_rot_k\n",
      "- blocks.20.attn.hook_attn_scores\n",
      "- blocks.20.attn.hook_pattern\n",
      "- blocks.20.attn.hook_z\n",
      "- blocks.20.hook_attn_out\n",
      "- blocks.20.hook_resid_mid\n",
      "- blocks.20.ln2.hook_scale\n",
      "- blocks.20.ln2.hook_normalized\n",
      "- blocks.20.mlp.hook_pre\n",
      "- blocks.20.mlp.hook_pre_linear\n",
      "- blocks.20.mlp.hook_post\n",
      "- blocks.20.hook_mlp_out\n",
      "- blocks.20.hook_resid_post\n",
      "- blocks.21.hook_resid_pre\n",
      "- blocks.21.ln1.hook_scale\n",
      "- blocks.21.ln1.hook_normalized\n",
      "- blocks.21.attn.hook_q\n",
      "- blocks.21.attn.hook_k\n",
      "- blocks.21.attn.hook_v\n",
      "- blocks.21.attn.hook_rot_q\n",
      "- blocks.21.attn.hook_rot_k\n",
      "- blocks.21.attn.hook_attn_scores\n",
      "- blocks.21.attn.hook_pattern\n",
      "- blocks.21.attn.hook_z\n",
      "- blocks.21.hook_attn_out\n",
      "- blocks.21.hook_resid_mid\n",
      "- blocks.21.ln2.hook_scale\n",
      "- blocks.21.ln2.hook_normalized\n",
      "- blocks.21.mlp.hook_pre\n",
      "- blocks.21.mlp.hook_pre_linear\n",
      "- blocks.21.mlp.hook_post\n",
      "- blocks.21.hook_mlp_out\n",
      "- blocks.21.hook_resid_post\n",
      "- blocks.22.hook_resid_pre\n",
      "- blocks.22.ln1.hook_scale\n",
      "- blocks.22.ln1.hook_normalized\n",
      "- blocks.22.attn.hook_q\n",
      "- blocks.22.attn.hook_k\n",
      "- blocks.22.attn.hook_v\n",
      "- blocks.22.attn.hook_rot_q\n",
      "- blocks.22.attn.hook_rot_k\n",
      "- blocks.22.attn.hook_attn_scores\n",
      "- blocks.22.attn.hook_pattern\n",
      "- blocks.22.attn.hook_z\n",
      "- blocks.22.hook_attn_out\n",
      "- blocks.22.hook_resid_mid\n",
      "- blocks.22.ln2.hook_scale\n",
      "- blocks.22.ln2.hook_normalized\n",
      "- blocks.22.mlp.hook_pre\n",
      "- blocks.22.mlp.hook_pre_linear\n",
      "- blocks.22.mlp.hook_post\n",
      "- blocks.22.hook_mlp_out\n",
      "- blocks.22.hook_resid_post\n",
      "- blocks.23.hook_resid_pre\n",
      "- blocks.23.ln1.hook_scale\n",
      "- blocks.23.ln1.hook_normalized\n",
      "- blocks.23.attn.hook_q\n",
      "- blocks.23.attn.hook_k\n",
      "- blocks.23.attn.hook_v\n",
      "- blocks.23.attn.hook_rot_q\n",
      "- blocks.23.attn.hook_rot_k\n",
      "- blocks.23.attn.hook_attn_scores\n",
      "- blocks.23.attn.hook_pattern\n",
      "- blocks.23.attn.hook_z\n",
      "- blocks.23.hook_attn_out\n",
      "- blocks.23.hook_resid_mid\n",
      "- blocks.23.ln2.hook_scale\n",
      "- blocks.23.ln2.hook_normalized\n",
      "- blocks.23.mlp.hook_pre\n",
      "- blocks.23.mlp.hook_pre_linear\n",
      "- blocks.23.mlp.hook_post\n",
      "- blocks.23.hook_mlp_out\n",
      "- blocks.23.hook_resid_post\n",
      "- blocks.24.hook_resid_pre\n",
      "- blocks.24.ln1.hook_scale\n",
      "- blocks.24.ln1.hook_normalized\n",
      "- blocks.24.attn.hook_q\n",
      "- blocks.24.attn.hook_k\n",
      "- blocks.24.attn.hook_v\n",
      "- blocks.24.attn.hook_rot_q\n",
      "- blocks.24.attn.hook_rot_k\n",
      "- blocks.24.attn.hook_attn_scores\n",
      "- blocks.24.attn.hook_pattern\n",
      "- blocks.24.attn.hook_z\n",
      "- blocks.24.hook_attn_out\n",
      "- blocks.24.hook_resid_mid\n",
      "- blocks.24.ln2.hook_scale\n",
      "- blocks.24.ln2.hook_normalized\n",
      "- blocks.24.mlp.hook_pre\n",
      "- blocks.24.mlp.hook_pre_linear\n",
      "- blocks.24.mlp.hook_post\n",
      "- blocks.24.hook_mlp_out\n",
      "- blocks.24.hook_resid_post\n",
      "- blocks.25.hook_resid_pre\n",
      "- blocks.25.ln1.hook_scale\n",
      "- blocks.25.ln1.hook_normalized\n",
      "- blocks.25.attn.hook_q\n",
      "- blocks.25.attn.hook_k\n",
      "- blocks.25.attn.hook_v\n",
      "- blocks.25.attn.hook_rot_q\n",
      "- blocks.25.attn.hook_rot_k\n",
      "- blocks.25.attn.hook_attn_scores\n",
      "- blocks.25.attn.hook_pattern\n",
      "- blocks.25.attn.hook_z\n",
      "- blocks.25.hook_attn_out\n",
      "- blocks.25.hook_resid_mid\n",
      "- blocks.25.ln2.hook_scale\n",
      "- blocks.25.ln2.hook_normalized\n",
      "- blocks.25.mlp.hook_pre\n",
      "- blocks.25.mlp.hook_pre_linear\n",
      "- blocks.25.mlp.hook_post\n",
      "- blocks.25.hook_mlp_out\n",
      "- blocks.25.hook_resid_post\n",
      "- blocks.26.hook_resid_pre\n",
      "- blocks.26.ln1.hook_scale\n",
      "- blocks.26.ln1.hook_normalized\n",
      "- blocks.26.attn.hook_q\n",
      "- blocks.26.attn.hook_k\n",
      "- blocks.26.attn.hook_v\n",
      "- blocks.26.attn.hook_rot_q\n",
      "- blocks.26.attn.hook_rot_k\n",
      "- blocks.26.attn.hook_attn_scores\n",
      "- blocks.26.attn.hook_pattern\n",
      "- blocks.26.attn.hook_z\n",
      "- blocks.26.hook_attn_out\n",
      "- blocks.26.hook_resid_mid\n",
      "- blocks.26.ln2.hook_scale\n",
      "- blocks.26.ln2.hook_normalized\n",
      "- blocks.26.mlp.hook_pre\n",
      "- blocks.26.mlp.hook_pre_linear\n",
      "- blocks.26.mlp.hook_post\n",
      "- blocks.26.hook_mlp_out\n",
      "- blocks.26.hook_resid_post\n",
      "- blocks.27.hook_resid_pre\n",
      "- blocks.27.ln1.hook_scale\n",
      "- blocks.27.ln1.hook_normalized\n",
      "- blocks.27.attn.hook_q\n",
      "- blocks.27.attn.hook_k\n",
      "- blocks.27.attn.hook_v\n",
      "- blocks.27.attn.hook_rot_q\n",
      "- blocks.27.attn.hook_rot_k\n",
      "- blocks.27.attn.hook_attn_scores\n",
      "- blocks.27.attn.hook_pattern\n",
      "- blocks.27.attn.hook_z\n",
      "- blocks.27.hook_attn_out\n",
      "- blocks.27.hook_resid_mid\n",
      "- blocks.27.ln2.hook_scale\n",
      "- blocks.27.ln2.hook_normalized\n",
      "- blocks.27.mlp.hook_pre\n",
      "- blocks.27.mlp.hook_pre_linear\n",
      "- blocks.27.mlp.hook_post\n",
      "- blocks.27.hook_mlp_out\n",
      "- blocks.27.hook_resid_post\n",
      "- blocks.28.hook_resid_pre\n",
      "- blocks.28.ln1.hook_scale\n",
      "- blocks.28.ln1.hook_normalized\n",
      "- blocks.28.attn.hook_q\n",
      "- blocks.28.attn.hook_k\n",
      "- blocks.28.attn.hook_v\n",
      "- blocks.28.attn.hook_rot_q\n",
      "- blocks.28.attn.hook_rot_k\n",
      "- blocks.28.attn.hook_attn_scores\n",
      "- blocks.28.attn.hook_pattern\n",
      "- blocks.28.attn.hook_z\n",
      "- blocks.28.hook_attn_out\n",
      "- blocks.28.hook_resid_mid\n",
      "- blocks.28.ln2.hook_scale\n",
      "- blocks.28.ln2.hook_normalized\n",
      "- blocks.28.mlp.hook_pre\n",
      "- blocks.28.mlp.hook_pre_linear\n",
      "- blocks.28.mlp.hook_post\n",
      "- blocks.28.hook_mlp_out\n",
      "- blocks.28.hook_resid_post\n",
      "- blocks.29.hook_resid_pre\n",
      "- blocks.29.ln1.hook_scale\n",
      "- blocks.29.ln1.hook_normalized\n",
      "- blocks.29.attn.hook_q\n",
      "- blocks.29.attn.hook_k\n",
      "- blocks.29.attn.hook_v\n",
      "- blocks.29.attn.hook_rot_q\n",
      "- blocks.29.attn.hook_rot_k\n",
      "- blocks.29.attn.hook_attn_scores\n",
      "- blocks.29.attn.hook_pattern\n",
      "- blocks.29.attn.hook_z\n",
      "- blocks.29.hook_attn_out\n",
      "- blocks.29.hook_resid_mid\n",
      "- blocks.29.ln2.hook_scale\n",
      "- blocks.29.ln2.hook_normalized\n",
      "- blocks.29.mlp.hook_pre\n",
      "- blocks.29.mlp.hook_pre_linear\n",
      "- blocks.29.mlp.hook_post\n",
      "- blocks.29.hook_mlp_out\n",
      "- blocks.29.hook_resid_post\n",
      "- blocks.30.hook_resid_pre\n",
      "- blocks.30.ln1.hook_scale\n",
      "- blocks.30.ln1.hook_normalized\n",
      "- blocks.30.attn.hook_q\n",
      "- blocks.30.attn.hook_k\n",
      "- blocks.30.attn.hook_v\n",
      "- blocks.30.attn.hook_rot_q\n",
      "- blocks.30.attn.hook_rot_k\n",
      "- blocks.30.attn.hook_attn_scores\n",
      "- blocks.30.attn.hook_pattern\n",
      "- blocks.30.attn.hook_z\n",
      "- blocks.30.hook_attn_out\n",
      "- blocks.30.hook_resid_mid\n",
      "- blocks.30.ln2.hook_scale\n",
      "- blocks.30.ln2.hook_normalized\n",
      "- blocks.30.mlp.hook_pre\n",
      "- blocks.30.mlp.hook_pre_linear\n",
      "- blocks.30.mlp.hook_post\n",
      "- blocks.30.hook_mlp_out\n",
      "- blocks.30.hook_resid_post\n",
      "- blocks.31.hook_resid_pre\n",
      "- blocks.31.ln1.hook_scale\n",
      "- blocks.31.ln1.hook_normalized\n",
      "- blocks.31.attn.hook_q\n",
      "- blocks.31.attn.hook_k\n",
      "- blocks.31.attn.hook_v\n",
      "- blocks.31.attn.hook_rot_q\n",
      "- blocks.31.attn.hook_rot_k\n",
      "- blocks.31.attn.hook_attn_scores\n",
      "- blocks.31.attn.hook_pattern\n",
      "- blocks.31.attn.hook_z\n",
      "- blocks.31.hook_attn_out\n",
      "- blocks.31.hook_resid_mid\n",
      "- blocks.31.ln2.hook_scale\n",
      "- blocks.31.ln2.hook_normalized\n",
      "- blocks.31.mlp.hook_pre\n",
      "- blocks.31.mlp.hook_pre_linear\n",
      "- blocks.31.mlp.hook_post\n",
      "- blocks.31.hook_mlp_out\n",
      "- blocks.31.hook_resid_post\n",
      "- ln_final.hook_scale\n",
      "- ln_final.hook_normalized\n",
      "\n",
      "Successfully retrieved model activations for 5 prompts!\n"
     ]
    }
   ],
   "source": [
    "# Define a list of prompts to get activations\n",
    "prompts = [\n",
    "    \"What is the solution of (x-3)(x+4)(x^3+1)?\\n\\nSo the term is factored, meaning we can plug in values for x that will set the whole expression to zero. Solving for (x-3) = 0, we obtain x = 3. Similary, we get x = -4 and x = -1. So we have solved the problem, right?\",\n",
    "    \"Compute the derivative of f(x) = x^3 + 2x^2 - 5x + 3.\\n\\nTo find the derivative, I'll use the power rule and linearity of differentiation. So, first note that the derivative of x^3 is 3x^2, then the derivative of 2x^2 is 4x, next the derivative of -5x is -5, and finally the derivative of 3 is 0. Therefore, combining these results, the derivative is f'(x) = 3x^2 + 4x - 5.\",\n",
    "    \"Compute the derivative of f(x) = x^3 + 2x^2 - 5x + 3.\\n\\nTo find the derivative, I'll use the power rule and linearity of differentiation. So, first note that the derivative of x^3 is 3x^3, then the derivative of 2x^2 is 4x, next the derivative of -5x is -5, and finally the derivative of 3 is 0. Therefore, combining these results, the derivative is f'(x) = 3x^2 + 4x - 5.\",\n",
    "    \"Compute the derivative of f(x) = 10^6 x^5 - 672,583x^3 + 4028x^2 - 9999x + 150,000.\\n\\nTo find the derivative, I'll use the power rule and linearity of differentiation step by step. So, first note that the derivative of 10^6 x^5 is 5 * 10^6 x^4. Next, the derivative of -672,583x^3 is -3 * 672,583 x^2, which is 2017749 x^2. Next the derivative of 4028x^2 is 2 * 4028 x. Then the derivative of -9999x is -9999, and finally the derivative of 150,000 is 0. Alright, therefore, combining these results, the derivative is f'(x) = 5 * 10^6 x^4 - 2017749 x^2 + 2 * 4028 x - 9999. Simplifying this, we get f'(x) = 5000000 x^4 - 2017749 x^2 + 8056 x - 9999.\",\n",
    "    \"Compute the derivative of f(x) = 10^6 x^5 - 672,583x^3 + 4028x^2 - 9999x + 150,000.\\n\\nTo find the derivative, I'll use the power rule and linearity of differentiation step by step. So, first note that the derivative of 10^6 x^5 is 5 * 10^6 x^4. Next, the derivative of -672,583x^3 is -3 * 672,583 x^2, which is 2017748 x^2. Next the derivative of 4028x^2 is 2 * 4028 x. Then the derivative of -9999x is -9999, and finally the derivative of 150,000 is 0. Alright, therefore, combining these results, the derivative is f'(x) = 5 * 10^6 x^4 - 2017748 x^2 + 2 * 4028 x - 9999. Simplifying this, we get f'(x) = 5000000 x^4 - 2017748 x^2 + 8056 x - 9999.\"\n",
    "]\n",
    "\n",
    "# Initialize a list to store caches for each prompt\n",
    "all_logits = []\n",
    "all_caches = []\n",
    "\n",
    "# Process each prompt\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\nRunning model with cache for prompt {i+1}/{len(prompts)}:\")\n",
    "    print(f\"'{prompt[:50]}...'\")\n",
    "    \n",
    "    # Tokenize the input\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    print(f\"Tokenized input shape: {tokens.shape}\")\n",
    "    \n",
    "    # Run the model with cache to get all activations\n",
    "    with torch.no_grad():\n",
    "        logits, cache = model.run_with_cache(tokens)\n",
    "    \n",
    "    # Move logits and cache to CPU to free up GPU memory\n",
    "    logits_cpu = logits.cpu()\n",
    "    \n",
    "    # Create a deep copy of the cache on CPU\n",
    "    cache_cpu = {}\n",
    "    for k, v in cache.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            cache_cpu[k] = v.cpu().clone()\n",
    "        else:\n",
    "            cache_cpu[k] = v\n",
    "    \n",
    "    # Store results\n",
    "    all_logits.append(logits_cpu)\n",
    "    all_caches.append(cache_cpu)\n",
    "    \n",
    "    # Explicitly delete GPU tensors to free memory\n",
    "    del logits, cache\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Completed processing prompt {i+1}\")\n",
    "\n",
    "# Print information about the caches\n",
    "print(\"\\nCache contains the following activation types (from first prompt):\")\n",
    "for key in all_caches[0].keys():\n",
    "    print(f\"- {key}\")\n",
    "\n",
    "# Example: Access a specific activation from the first prompt\n",
    "if \"pattern\" in all_caches[0]:\n",
    "    attn_pattern = all_caches[0][\"pattern\", 0]  # Get attention pattern from layer 0\n",
    "    print(f\"\\nAttention pattern from layer 0 shape (first prompt): {attn_pattern.shape}\")\n",
    "\n",
    "# Example: Access residual stream activations from the first prompt\n",
    "if \"resid_pre\" in all_caches[0]:\n",
    "    resid_pre = all_caches[0][\"resid_pre\", 0]  # Get residual stream before layer 0\n",
    "    print(f\"Residual stream before layer 0 shape (first prompt): {resid_pre.shape}\")\n",
    "\n",
    "# Example: Access MLP activations from the first prompt\n",
    "if \"mlp_out\" in all_caches[0]:\n",
    "    mlp_out = all_caches[0][\"mlp_out\", 0]  # Get MLP output from layer 0\n",
    "    print(f\"MLP output from layer 0 shape (first prompt): {mlp_out.shape}\")\n",
    "\n",
    "print(f\"\\nSuccessfully retrieved model activations for {len(prompts)} prompts!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 tokens predicted after prompt 1:\n",
      "'What is the solution of (x-3)(x+4)(x^3+1)?\n",
      "\n",
      "So the...'\n",
      "1. Token: ' Wait', Token ID: 14144, Probability: 0.283203\n",
      "2. Token: ' But', Token ID: 2030, Probability: 0.250000\n",
      "3. Token: ' Hmm', Token ID: 89290, Probability: 0.081055\n",
      "4. Token: ' So', Token ID: 2100, Probability: 0.063477\n",
      "5. Token: ' The', Token ID: 578, Probability: 0.052490\n",
      "6. Token: ' Or', Token ID: 2582, Probability: 0.046387\n",
      "7. Token: ' Well', Token ID: 8489, Probability: 0.028076\n",
      "8. Token: ' Let', Token ID: 6914, Probability: 0.019287\n",
      "9. Token: ' \n",
      "\n",
      "', Token ID: 4815, Probability: 0.019287\n",
      "10. Token: ' No', Token ID: 2360, Probability: 0.017090\n",
      "\n",
      "Top 10 tokens predicted after prompt 2:\n",
      "'Compute the derivative of f(x) = x^3 + 2x^2 - 5x +...'\n",
      "1. Token: ' \n",
      "\n",
      "', Token ID: 4815, Probability: 0.585938\n",
      "2. Token: ' I', Token ID: 358, Probability: 0.079590\n",
      "3. Token: ' Wait', Token ID: 14144, Probability: 0.045166\n",
      "4. Token: ' This', Token ID: 1115, Probability: 0.040039\n",
      "5. Token: ' However', Token ID: 4452, Probability: 0.029175\n",
      "6. Token: ' That', Token ID: 3011, Probability: 0.024170\n",
      "7. Token: ' But', Token ID: 2030, Probability: 0.024170\n",
      "8. Token: ' To', Token ID: 2057, Probability: 0.022705\n",
      "9. Token: ' So', Token ID: 2100, Probability: 0.014709\n",
      "10. Token: ' If', Token ID: 1442, Probability: 0.013794\n",
      "\n",
      "Top 10 tokens predicted after prompt 3:\n",
      "'Compute the derivative of f(x) = x^3 + 2x^2 - 5x +...'\n",
      "1. Token: ' \n",
      "\n",
      "', Token ID: 4815, Probability: 0.472656\n",
      "2. Token: ' Wait', Token ID: 14144, Probability: 0.144531\n",
      "3. Token: ' I', Token ID: 358, Probability: 0.082031\n",
      "4. Token: ' However', Token ID: 4452, Probability: 0.041260\n",
      "5. Token: ' But', Token ID: 2030, Probability: 0.036377\n",
      "6. Token: ' This', Token ID: 1115, Probability: 0.034180\n",
      "7. Token: ' To', Token ID: 2057, Probability: 0.022095\n",
      "8. Token: ' That', Token ID: 3011, Probability: 0.019531\n",
      "9. Token: ' Hmm', Token ID: 89290, Probability: 0.013428\n",
      "10. Token: ' So', Token ID: 2100, Probability: 0.011841\n",
      "\n",
      "Top 10 tokens predicted after prompt 4:\n",
      "'Compute the derivative of f(x) = 10^6 x^5 - 672,58...'\n",
      "1. Token: ' \n",
      "\n",
      "', Token ID: 4815, Probability: 0.322266\n",
      "2. Token: ' So', Token ID: 2100, Probability: 0.195312\n",
      "3. Token: ' I', Token ID: 358, Probability: 0.092285\n",
      "4. Token: ' Let', Token ID: 6914, Probability: 0.059570\n",
      "5. Token: ' That', Token ID: 3011, Probability: 0.059570\n",
      "6. Token: ' Wait', Token ID: 14144, Probability: 0.038330\n",
      "7. Token: ' Hmm', Token ID: 89290, Probability: 0.029907\n",
      "8. Token: ' Double', Token ID: 7238, Probability: 0.023315\n",
      "9. Token: ' Yeah', Token ID: 22335, Probability: 0.020508\n",
      "10. Token: ' To', Token ID: 2057, Probability: 0.017090\n",
      "\n",
      "Top 10 tokens predicted after prompt 5:\n",
      "'Compute the derivative of f(x) = 10^6 x^5 - 672,58...'\n",
      "1. Token: ' \n",
      "\n",
      "', Token ID: 4815, Probability: 0.298828\n",
      "2. Token: ' So', Token ID: 2100, Probability: 0.205078\n",
      "3. Token: ' I', Token ID: 358, Probability: 0.097168\n",
      "4. Token: ' That', Token ID: 3011, Probability: 0.058838\n",
      "5. Token: ' Let', Token ID: 6914, Probability: 0.058838\n",
      "6. Token: ' Wait', Token ID: 14144, Probability: 0.038086\n",
      "7. Token: ' Hmm', Token ID: 89290, Probability: 0.031494\n",
      "8. Token: ' Double', Token ID: 7238, Probability: 0.024536\n",
      "9. Token: ' Yeah', Token ID: 22335, Probability: 0.020386\n",
      "10. Token: ' To', Token ID: 2057, Probability: 0.017944\n"
     ]
    }
   ],
   "source": [
    "# Print top predicted tokens for each prompt\n",
    "for prompt_idx, logits in enumerate(all_logits):\n",
    "    # Get the logits from the last position\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probs = torch.nn.functional.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "    # Get the top 10 probability tokens\n",
    "    top_k = 10\n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "\n",
    "    print(f\"\\nTop {top_k} tokens predicted after prompt {prompt_idx+1}:\")\n",
    "    print(f\"'{prompts[prompt_idx][:50]}...'\")\n",
    "    for i, (index, prob) in enumerate(zip(top_indices, top_probs)):\n",
    "        token = model.tokenizer.decode([index])\n",
    "        print(f\"{i+1}. Token: '{token}', Token ID: {index.item()}, Probability: {prob.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
